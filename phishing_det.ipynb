{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phishing Detection Using NLP Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/spam.csv\", encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1).rename(columns={'v1':'labels', 'v2':'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'].value_counts().plot(kind='pie',autopct='%.0f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_func import TextClean\n",
    "tc= TextClean()\n",
    "df['text']=tc.fit_transform(df['text'])\n",
    "\n",
    "corpus = ' '.join([j for i in df['text'].values for j in i.split(' ') if len(j)>2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Size of corpus is {len(corpus.split(' '))} words and {len(corpus)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(width = 1200, height = 1200,\n",
    "                background_color ='white',\n",
    "                min_font_size = 10)\n",
    "word_cloud = wc.generate(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.title(\"Word Cloud for Text\", fontsize=20)\n",
    "plt.imshow(word_cloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'],df['labels'],stratify=df['labels'], test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train=le.fit_transform(y_train)\n",
    "y_test=le.transform(y_test)\n",
    "\n",
    "count = CountVectorizer()\n",
    "X_train=count.fit_transform(X_train)\n",
    "X_test= count.transform(X_test)\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "X_train=tfidf.fit_transform(X_train)\n",
    "X_test= tfidf.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train,y_train)\n",
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, precision_score, accuracy_score, recall_score\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating and setting up a MLFlow experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "if mlflow.get_experiment_by_name('Phishing_Detection'):\n",
    "    mlflow.delete_experiment(mlflow.get_experiment_by_name('Phishing_Detection').experiment_id)\n",
    "experiment = mlflow.create_experiment('Phishing_Detection')\n",
    "mlflow.set_experiment(experiment_name='Phishing_Detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y=le.fit_transform(df['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'],y,stratify=df['labels'], test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Text Preprocessing Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Pipeline([('textprep',TextClean()),\n",
    "                          ('cv',CountVectorizer()),\n",
    "                          ('tfidf',TfidfTransformer())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyper-parameter ptimization with hyperopt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK\n",
    "from hyperopt.pyll import scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import ColSpec, Schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "     with mlflow.start_run(nested = True):\n",
    "         \n",
    "        classifier_type = params['type']\n",
    "        del params['type']\n",
    "        \n",
    "        if classifier_type == 'gb':\n",
    "            clf = GradientBoostingClassifier(**params)\n",
    "        elif classifier_type == 'rf':\n",
    "            clf = RandomForestClassifier(**params)\n",
    "        elif classifier_type == 'nb':\n",
    "            clf = MultinomialNB(**params)\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "        pipeline = Pipeline(steps = [('preprocessor', preprocessor), ('model', clf)])\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        predictions =  pipeline.predict(X_test) \n",
    "    \n",
    "        train_accuracy_score =  pipeline.score(X_train, y_train)\n",
    "        test_accuracy_score = accuracy_score(y_test, predictions)\n",
    "        test_precision_score = precision_score(y_test, predictions)\n",
    "        test_recall_score = recall_score(y_test, predictions)\n",
    "        test_f1_score = f1_score(y_test, predictions)\n",
    "\n",
    "        metrics = {\n",
    "            'Train_accuracy_score': train_accuracy_score, \n",
    "            'Test_accuracy_score': test_accuracy_score,\n",
    "            'Test_precision_score': test_precision_score,\n",
    "            'Test_recall_score': test_recall_score,\n",
    "            'Test_f1_score': test_f1_score \n",
    "        }\n",
    "\n",
    "        mlflow.log_metrics(metrics)\n",
    "\n",
    "        input = Schema([ColSpec('string','text')])\n",
    "        output = Schema([ColSpec('integer')])\n",
    "        signature = ModelSignature(inputs=input,outputs=output)\n",
    "        mlflow.sklearn.log_model(pipeline, \n",
    "                                 f'clf_hpo_{classifier_type}',\n",
    "                                 signature = signature)\n",
    "        mlflow.set_tags({'model':f'clf_hpo_{classifier_type}',\n",
    "                         'ac':test_accuracy_score})\n",
    "\n",
    "        return {'loss': -test_f1_score, 'status': STATUS_OK}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = hp.choice('classifier_type', [\n",
    "    {\n",
    "        'type': 'gb',\n",
    "        'n_estimators':scope.int(hp.quniform('n_estimators_gb', 100, 500, 50)),\n",
    "        'loss': hp.choice('loss', ['log_loss', 'exponential']),\n",
    "        'criterion': hp.choice('criterion', ['friedman_mse', 'squared_error']),\n",
    "        'max_depth': scope.int(hp.quniform('max_depth', 4, 15, 1)),\n",
    "        'min_samples_leaf': scope.int(hp.uniform('min_samples_leaf_gb',1,5)),\n",
    "        'min_samples_split': scope.int(hp.uniform('min_samples_split_gb',2,6))\n",
    "    },\n",
    "    {\n",
    "        'type': 'rf',\n",
    "        'n_estimators':scope.int(hp.quniform('n_estimators_rf', 100, 500, 50)),\n",
    "        'max_depth': scope.int(hp.quniform('max_depth_rf', 4, 15, 1)),\n",
    "        'min_samples_leaf': scope.int(hp.uniform('min_samples_leaf_rf',1,5)),\n",
    "        'min_samples_split': scope.int(hp.uniform('min_samples_split_rf',2,6))\n",
    "    },\n",
    "    {\n",
    "        'type': 'nb',\n",
    "        'alpha': hp.lognormal('alpha', 0, 1.0),\n",
    "        'force_alpha': hp.choice('force_alpha', [True, False])\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = tpe.suggest\n",
    "\n",
    "with mlflow.start_run():\n",
    "    best_result = fmin(\n",
    "        fn = objective, \n",
    "        space = search_space,\n",
    "        algo = algo,\n",
    "        max_evals = 32,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "print(hyperopt.space_eval(search_space, best_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperopt.space_eval(search_space, best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params=hyperopt.space_eval(search_space, best_result)\n",
    "del params['type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Registering best model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs =json.loads(mlflow.search_runs(mlflow.get_experiment_by_name('Phishing_Detection').experiment_id).sort_values('metrics.Test_f1_score',ascending=False)['tags.mlflow.log-model.history'][0][1:-1])\n",
    "model_name=runs['artifact_path']\n",
    "run_id=runs['run_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting Prediction from best model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = f\"runs:/{run_id}/{model_name}\"\n",
    "loaded_model = mlflow.sklearn.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loaded_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=loaded_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, precision_score, accuracy_score, recall_score\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.register_model(model_uri, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = mlflow.MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.update_model_version(\n",
    "    name = model_name,\n",
    "    version = 1,\n",
    "    description = 'This model had the best accuracy score '\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.transition_model_version_stage(\n",
    "  name = model_name,\n",
    "  version = 1,\n",
    "  stage = 'Production'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Serve the model locally (make sure to replace the right run id)\n",
    "\n",
    "`mlflow models serve -m <model_uri> --env-manager local --host 127.0.0.1:1234`\n",
    "\n",
    "- Now open up a new tab\n",
    "\n",
    "`curl -X POST -H \"Content-Type:application/json\" --data '{\"dataframe_split\": {\"columns\":['text],\"data\":[\"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\"]}}' http://127.0.0.1:1234/invocations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phishing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
